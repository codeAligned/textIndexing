\section{Compressed Bitvector}

\begin{Definition}
  \label{def:zerothOrderEntropy}
  Given a sequence $X$ of length $n$ over an alphabet $\Sigma$. Let $n_c$ be the number of occurrences of character $c \in \Sigma$ in $X$.
  \begin{align}
    \mathcal{H}_0(X) := \sum\limits_{\substack{c \in \Sigma\\ n_c > 0}} \frac{n_c}{n}\log\frac{n}{n_c}
  \end{align}
  is called the \defi{zeroth order empirical entropy}{$\mathcal{H}_0$} and provides lower bound for the number of bits needed to compress $X$ using a compressor which just considers character frequencies.
\end{Definition}

\subsection{Elias-Fano Encoded Bitvector}

\begin{Theorem}
  \label{thm:eliasFanoEncoding}
  Given a non-decreasing sequence $X$ of length $m$ over the alphabet $[0,n]$. Sequence $X$ can be compressed using $2m + m\log\frac{n}{m} + o(m)$ bits while each element can still be accessed in constant time. This is known as \defi{Elias-Fano-Encoding}{Elias-Fano-Encoding}.
\end{Theorem}

\begin{Proof}
  Divide each element into a high-part and a low-part: The first $\lfloor \log m \rfloor$ bits correspond to the high-part, the other $\lceil \log n \rceil - \lfloor \log m \rfloor$ bits correspond to the low-part. The sequence of high-parts of $X$ is also non-decreasing. We use a unary gap encoding to represent the gaps and store the result in a bitvector $H$. For a gap of size $\delta_i$ we use $\delta_i + 1$ bits ($\delta_i$ zeros and $1$ one). The sum of the gaps (the total number of zeros in $H$) is at most $2^{\lfloor \log m \rfloor} \leq 2^{\log m} = m$. Therefore $H$ has size at most $2m$ (\#zeros + \#ones). The low parts can be stored explicitly.

  \begin{algorithm}[htb]
    \begin{codebox}
      \Procname{$\proc{Elias-Fano-Access}(i)$}
      \li $p \gets \proc{Select}_1(i + 1, H)$
      \li $x \gets p - i$
      \li \Return  $x \cdot 2^{\lceil\log n\rceil - \lfloor\log m\rfloor} + L[i]$
    \end{codebox}
    \caption{Access to the $i$-th element of \id{X}.}
    \label{alg:eliasFanoAccess}
  \end{algorithm}

  Algorithm~\ref{alg:eliasFanoAccess} provides a method to access the $i$-th element of the sequence in constant time.
\end{Proof}

Theorem~\ref{thm:eliasFanoEncoding} can be used to compress a bitvector~$B$. Let~$n$ be the length of~$B$ and~$m$ be the number of set bits. Further let $X$ be the positions of the set bits. $X$ forms an increasing sequence and therefore be used for compression.

\begin{Example}
  \begin{table}[htbp]
    \centering
    \begin{tabular}{ccccccccc}
      \toprule
      $X$ & $=$ & $4$ & $13$ & $15$ & $24$ & $26$ & $27$ & $29$ \\
      \midrule
      & &
      \texttt{00|100} &
      \texttt{01|101} &
      \texttt{01|111} &
      \texttt{11|000} &
      \texttt{11|010} &
      \texttt{11|011} &
      \texttt{11|101} \\

      $\delta$ & $=$ & $0$ & $1$ & $0$ & $2$ & $0$ & $0$ & $0$ \\
      $H$ & $=$ & \multicolumn{7}{l}{\texttt{1011001111}} \\
      $L$ & $=$ & \multicolumn{7}{l}{\texttt{4, 5, 7, 0, 2, 3, 5}} \\
      \bottomrule
    \end{tabular}
    \caption{Elias-Fano-Encoding for a given sequence $X$.}
    \label{tbl:eliasFanoExample}
  \end{table}
  Table~\ref{tbl:eliasFanoExample} shows how the bitvector $B=(4,13,15,24,26,27,29)$ gets encoded. The length of $B$ is $7$, so $\lfloor \log 7 \rfloor = 2$ are in the high-part and $\lceil \log 29 \rceil - \lfloor \log 7 \rfloor = 3$ bits are in the low-part. The second row shows the binary representation of each elements, the vertical bar separates the low- and the high-part. Line $\delta$ shows the differences between two consecutive high-parts. Lines $H$ and $L$ now show the compressed high-parts and the explicitly stored low-parts.
\end{Example}

\subsection{$\mathcal{H}_0$-Compressed Bitvector}

Let $B$ be a bitvector of length $n$ with $\kappa$ bits set. The entropy of this bitvector is
\begin{align}
  \mathcal{H}_0(B) = \frac{\kappa}{n}\log\frac{n}{\kappa} + \frac{n-\kappa}{n}\log\frac{n}{n - \kappa}
  \text{.}
\end{align}

\begin{Theorem}
  A bitvector can be represented in $n\mathcal{H}_0(B) + o(n)$ bits space while \proc{Rank} and \proc{Select} queries can be executed in constant time.
\end{Theorem}

\begin{Proof}
  Split the bitvector into blocks of length $K = \frac{1}{2}\log n$ bits. For each block $i$ we store the number of set bits $\kappa_i$ using $\lceil \log K + 1\rceil$ bits. These identifiers sum up to $\mathcal{O}(n\frac{\log\log n}{\log n})$ bits.

  Now we represent each block as a tuple $(\kappa_i,r_i)$. The first element $0 \leq \kappa_i \leq K$ is the number of set bits in this block. The second element $0 \leq r_i < \binom{K}{\kappa_i}$ is the index within the class.
  \begin{align}
    \begin{aligned}
      \vert B \vert
      &= \sum\limits_{i=0}^\frac{n-1}{K} \left\lceil\log\binom{K}{\kappa_i}\right\rceil \\
      &\leq \log\left(\prod\limits_{i=0}^{\frac{n-1}{K}}\binom{K}{\kappa_i}\right) + \left\lceil\frac{n}{K}\right\rceil \\
      &\leq \log\binom{n}{\kappa_0 + \ldots + \kappa_{(n-1)/K}} + \left\lceil\frac{n}{K}\right\rceil \\
      &= \log\binom{n}{K}  + \left\lceil\frac{n}{K}\right\rceil\\
      &\leq n\mathcal{H}_0(B) + \left\lceil\frac{n}{K}\right\rceil \\
      &= n\mathcal{H}_0(B) + \mathcal{O}\left(\frac{n}{\log n}\right)
    \end{aligned}
  \end{align}
  In total this gives a space requirement of $n\mathcal{H}_0(B) + \mathcal{O}\left(\frac{n}{\log n}\right) + \mathcal{O}(n\frac{\log\log n}{\log n}) = n\mathcal{H}_0(B) + o(n)$ bits. We will not go into the implementation of \proc{Rank} and \proc{Select} here.
\end{Proof}

To encode and decode a block with $\kappa$ bits set we must be able to transform the bitstring into the $(\kappa, r)$ pair and vice versa. We can do this on-the-fly by using a \defi{combinatorial number system}{Combinatorial Number System}. The idea is to give them consecutive numbers from $0$ to $\binom{K}{\kappa}-1$ in the order given by their values interpreted as binary numbers.

% TODO (pjungeblut): This can nicely be visualised.
\begin{itemize}
  \item To encode a given bitstring initialize $r = 0$ and go through the bits from left to right. If the first bit is a $0$, we just drop it, decrement $K$ and continue with the remaining bits. If the first bit is a $1$, we know that there is $\binom{K-1}{\kappa-1}$ other possible bitstrings that start with $0$ and contain $\kappa$ bits. Therefore we increase $r$ by $\binom{K-1}{\kappa-1}$, decrement $K$ and $\kappa$ and continue with the next bit.
  \item To decode a given pair $(\kappa, r)$, we check if $r \geq \binom{K}{\kappa}$. If so, the first bit is a $1$ and we subtract $\binom{K}{\kappa}$ from $r$, decrease $K$ and $\kappa$ and continue to get the next bit. If $r < \binom{K}{\kappa}$, then the first bit is $0$ and we decrease $K$ and continue to get the next bit.
\end{itemize}
